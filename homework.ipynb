{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PS3: Neural Networks for Classification and Natural Language Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this task is to gain an understanding of training neural networks. Likewise, you will get to learn about the pytorch framework.\n",
    "\n",
    "## Submission Instructions\n",
    "\n",
    "After completing the exercises below, generate a pdf of the code **with** outputs. After that create a zip file containing both the completed exercise and the generated PDF. You are **required** to check the PDF to make sure all the code **and** outputs are clearly visible and easy to read. If your code goes off the page, you should reduce the line size. I generally recommend not going over 80 characters.\n",
    "\n",
    "Finally, name the zip file using a combination of your the assigment and your name, e.g., ps3_rios.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART I: Data Cleaning (10 points)\n",
    "\n",
    "Load the \"surnames.csv\" file to train a LSTM to predict nationality based on surname. You will need to transform the data from a list of strings to a list of indexes. For example, the following data\n",
    "\n",
    "```\n",
    "Anthony\n",
    "John\n",
    "David\n",
    "```\n",
    "\n",
    "should be transformed into a list of lists.\n",
    "\n",
    "```\n",
    "[[0, 1, 2, 3, 4, 1, 5],\n",
    " [6, 4, 3, 1],\n",
    " [7, 8, 9, 10, 11]]\n",
    "```\n",
    "\n",
    "Next, you will need zero-pad all examples to be the same size.\n",
    "\n",
    "```\n",
    "[[0, 1, 2, 3, 4, 1, 5],\n",
    " [6, 4, 3, 1, 0, 0, 0],\n",
    " [7, 8, 9, 10, 11, 0, 0]]\n",
    "```\n",
    "\n",
    "Finally, everything will be converted into numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '-': 1, 'á': 2, 't': 3, 's': 4, 'K': 5, 'w': 6, 'C': 7, 'ż': 8, 'ê': 9, 'S': 10, 'R': 11, 'o': 12, 'x': 13, 'c': 14, 'Á': 15, 'v': 16, 'l': 17, 'ó': 18, 'q': 19, 'f': 20, 'a': 21, ',': 22, 'Q': 23, 'm': 24, 'ú': 25, 'M': 26, 'ł': 27, 'A': 28, 'B': 29, 'P': 30, 'I': 31, 'H': 32, 'X': 33, 'T': 34, 'k': 35, 'J': 36, 'í': 37, 'h': 38, 'y': 39, 'é': 40, 'g': 41, 'ã': 42, 'õ': 43, 'V': 44, 'd': 45, 'n': 46, 'O': 47, ' ': 48, 'u': 49, 'à': 50, 'F': 51, 'ñ': 52, 'r': 53, 'e': 54, 'Ś': 55, 'D': 56, 'Z': 57, 'ö': 58, 'z': 59, '/': 60, 'i': 61, 'ì': 62, 'U': 63, 'ù': 64, 'j': 65, 'Y': 66, 'G': 67, 'E': 68, 'ü': 69, 'p': 70, 'L': 71, 'ą': 72, 'N': 73, 'è': 74, 'ß': 75, 'ò': 76, 'W': 77, 'b': 78, \"'\": 79, 'ń': 80, 'ä': 81, '1': 82}\n",
      "{'french': 0, 'dutch': 1, 'german': 2, 'chinese': 3, 'scottish': 4, 'spanish': 5, 'greek': 6, 'korean': 7, 'vietnamese': 8, 'russian': 9, 'italian': 10, 'portuguese': 11, 'english': 12, 'japanese': 13, 'arabic': 14, 'polish': 15, 'irish': 16, 'czech': 17}\n",
      "{0: 'french', 1: 'dutch', 2: 'german', 3: 'chinese', 4: 'scottish', 5: 'spanish', 6: 'greek', 7: 'korean', 8: 'vietnamese', 9: 'russian', 10: 'italian', 11: 'portuguese', 12: 'english', 13: 'japanese', 14: 'arabic', 15: 'polish', 16: 'irish', 17: 'czech'}\n"
     ]
    }
   ],
   "source": [
    "char2index = {'<PAD>': 0}\n",
    "index2char = {0: '<PAD>'}\n",
    "class2index = {} # stores the class index pairs.\n",
    "index2class = {}\n",
    "doc_lengths = [] # Stores the lengths of all docs (train, test and dev)\n",
    "X_train = [] # stores an \n",
    "y_train = [] # stores an index to the correct class\n",
    "X_dev = []\n",
    "y_dev = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "X_train_len = [] # Stores the length of each training name\n",
    "X_test_len = [] # ... length of each test name\n",
    "X_dev_len = [] # ... length of each dev name\n",
    "\n",
    "# Write code to load data here.\n",
    "dataset_filename = 'surnames.csv'\n",
    "datadir = 'data'\n",
    "dataset_path = os.path.join(datadir, dataset_filename)\n",
    "data_names = ['X_train', 'X_dev', 'X_test']\n",
    "\n",
    "def access_data(path):\n",
    "    ret = {'X_train':[], 'y_train':[], 'X_dev':[], 'y_dev':[], 'X_test':[], 'y_test':[], 'len':[]}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f,dialect='excel')\n",
    "        for i, row in enumerate(reader):\n",
    "#             if i > 5:\n",
    "#                 break\n",
    "#             print(row)\n",
    "            if row[0] == 'train':\n",
    "                ret['X_train'].append(row[1])\n",
    "                ret['y_train'].append(row[2])\n",
    "            if row[0] == 'test':\n",
    "                ret['X_test'].append(row[1])\n",
    "                ret['y_test'].append(row[2])\n",
    "            if row[0] == 'dev':\n",
    "                ret['X_dev'].append(row[1])\n",
    "                ret['y_dev'].append(row[2])\n",
    "            ret['len'].append(len(row[1]))\n",
    "    return ret\n",
    "    \n",
    "def load_data(path):\n",
    "    \n",
    "    results = access_data(path)\n",
    "    class2index = {}\n",
    "    for class_name in set(results['y_train']+results['y_dev']+results['y_test']):\n",
    "        class2index[class_name] = len(class2index)\n",
    "    index2class = {ind:class_name for class_name,ind in class2index.items()}\n",
    "    return results, class2index, index2class\n",
    "    \n",
    "    \n",
    "def update_mappings(X, x2index, index2x, map_elements=True):\n",
    "    if map_elements:\n",
    "        xs = set([element for x in X for element in x])\n",
    "    else:\n",
    "        xs = set([x for x in X])\n",
    "    for x in xs:\n",
    "#         print(\"\\nx:\",x)\n",
    "#         print(len(x2index))\n",
    "        x2index[x] = len(x2index)\n",
    "        index2x[len(index2x)] = x\n",
    "    \n",
    "        \n",
    "def convert_to_index_map(X, x2index, map_element=True):\n",
    "    index_mappings = []\n",
    "    for x in X:\n",
    "        if map_element:\n",
    "            index_map = [x2index[element] if element in x2index else 0 for element in x]\n",
    "        else:\n",
    "            if x in x2index:\n",
    "                index_map = x2index[x]\n",
    "            else:\n",
    "                print('S')\n",
    "                index_map = 0\n",
    "        index_mappings.append(index_map)\n",
    "    return index_mappings\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "data, class2index, index2class = load_data(dataset_path)\n",
    "X_train, y_train, X_dev, y_dev, X_test, y_test = data['X_train'],data['y_train'],data['X_dev'],data['y_dev'] \\\n",
    "                                                 ,data['X_test'],data['y_test']\n",
    "doc_lengths = data['len']\n",
    "d = [X_train, X_dev, X_test]\n",
    "dl = [X_train_len, X_dev_len, X_test_len]\n",
    "[dl[i].append(len(x)) for i in range(len(d)) for x in d[i]]\n",
    "update_mappings(X_train, char2index, index2char)\n",
    "# print(class2index)\n",
    "# update_mappings(y_train, class2index, index2class, map_elements=False)\n",
    "print(char2index)\n",
    "print(class2index)\n",
    "print(index2class)\n",
    "\n",
    "X_train_nums, X_dev_nums = convert_to_index_map(X_train, char2index),convert_to_index_map(X_dev, char2index)\n",
    "X_test_nums = convert_to_index_map(X_test, char2index)\n",
    "y_train = convert_to_index_map(y_train, class2index, map_element=False)\n",
    "y_dev = convert_to_index_map(y_dev, class2index, map_element=False)\n",
    "y_test = convert_to_index_map(y_test, class2index, map_element=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3060\n",
      "3060\n",
      "20074\n",
      "[30, 53, 61, 35, 21, 59, 14, 38, 61, 35, 12, 16]\n",
      "Prikazchikov\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "print(len(X_dev))\n",
    "print(len(X_dev_len))\n",
    "print(len(doc_lengths))\n",
    "print(X_train_nums[1])\n",
    "print(X_train[1])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longest in training set: 20\n",
      "3060\n",
      "3060\n",
      "(15000, 20)\n"
     ]
    }
   ],
   "source": [
    "# PADDING\n",
    "\n",
    "max_seq_len = max(doc_lengths)\n",
    "len_to_pad = len(max(X_train, key=lambda x: len(x)))\n",
    "print('longest in training set:', len_to_pad)\n",
    "X_train_eq_size = []\n",
    "X_dev_eq_size = []\n",
    "X_test_eq_size = []\n",
    "\n",
    "def pad_example(ex, len_to_pad, pad):\n",
    "    padded = ex[:len_to_pad] +[pad]*(len_to_pad -len(ex))\n",
    "    return padded\n",
    "# Write code to append data to code here\n",
    "for x in X_train_nums:\n",
    "    X_train_eq_size.append(pad_example(x,len_to_pad, 0))\n",
    "    \n",
    "for x in X_dev_nums:\n",
    "    X_dev_eq_size.append(pad_example(x,len_to_pad, 0))\n",
    "    \n",
    "for x in X_test_nums:\n",
    "    X_test_eq_size.append(pad_example(x,len_to_pad, 0))\n",
    "    \n",
    "print(len(X_dev))\n",
    "X_train = np.array(X_train_eq_size)\n",
    "X_dev = np.array(X_dev_eq_size)\n",
    "X_test = np.array(X_test_eq_size)\n",
    "print(len(X_dev))\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_dev = np.array(y_dev)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "X_train_len = np.array(X_train_len)\n",
    "X_dev_len = np.array(X_dev_len)\n",
    "X_test_len = np.array(X_test_len)\n",
    "\n",
    "idx = np.argsort(X_dev_len)[::-1]\n",
    "X_dev = X_dev[idx]\n",
    "y_dev = y_dev[idx]\n",
    "X_dev_len = X_dev_len[idx]\n",
    "\n",
    "idx = np.argsort(X_test_len)[::-1]\n",
    "X_test = X_test[idx]\n",
    "y_test = y_test[idx]\n",
    "X_test_len = X_test_len[idx]\n",
    "\n",
    "doc_lengths = np.array(doc_lengths)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART II: Classification (25 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, nb_layers, word2index, class2index, nb_lstm_units=100,\n",
    "                 embedding_dim=3, batch_size=3):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.vocab = word2index\n",
    "        self.tags = class2index\n",
    "\n",
    "        self.nb_layers = nb_layers\n",
    "        self.nb_lstm_units = nb_lstm_units\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.nb_tags = len(self.tags)\n",
    "\n",
    "        # build actual NN\n",
    "        self.__build_model()\n",
    "\n",
    "    def __build_model(self):\n",
    "        nb_vocab_words = len(self.vocab)\n",
    "\n",
    "        padding_idx = self.vocab['<PAD>']\n",
    "        self.word_embedding = nn.Embedding(\n",
    "            num_embeddings=nb_vocab_words,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.nb_lstm_units,\n",
    "            num_layers=self.nb_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.hidden_to_tag = nn.Linear(self.nb_lstm_units*self.nb_layers, self.nb_tags)\n",
    "        \n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.inference = False\n",
    "\n",
    "    def init_hidden(self, X):\n",
    "        # Initial ht (hidden state) and ct (context)\n",
    "        h0 = torch.zeros(self.nb_layers, X.size(0), self.nb_lstm_units).float()\n",
    "        c0 = torch.zeros(self.nb_layers, X.size(0), self.nb_lstm_units).float()\n",
    "        return (h0,c0)\n",
    "\n",
    "    def forward(self, X, X_lengths):\n",
    "        # reset the LSTM hidden state. Must be done before you run a new batch.\n",
    "        # Otherwise the LSTM will treat\n",
    "        # a new batch as a continuation of a sequence\n",
    "#         print('X start:')\n",
    "#         print(X)\n",
    "        self.hidden = self.init_hidden(X)\n",
    "#         print('hidden')\n",
    "#         print(self.hidden)\n",
    "#         print(self.hidden[0].shape)\n",
    "        batch_size, seq_len = X.shape\n",
    "        \n",
    "        # ---------------------\n",
    "        # 1. embed the input\n",
    "        # Dim transformation: (batch_size, seq_len) -> (batch_size, seq_len, embedding_dim)\n",
    "        X = self.word_embedding(X)\n",
    "#         print('after embedding')\n",
    "#         print(X)\n",
    "        # ---------------------\n",
    "        # 2. Run through RNN\n",
    "        # TRICK 2 ********************************\n",
    "        # Dim transformation: (batch_size, seq_len, embedding_dim) -> (batch_size, seq_len, nb_lstm_units)\n",
    "\n",
    "        # pack_padded_sequence so that padded items in the sequence won't be shown to the LSTM\n",
    "        X = torch.nn.utils.rnn.pack_padded_sequence(X, X_lengths, batch_first=True)\n",
    "\n",
    "        # now run through LSTM\n",
    "        # X contains the padded sequence output and ht contains the final hidden states\n",
    "        # for example.\n",
    "        X, (ht, ct) = self.lstm(X, self.hidden)\n",
    "        \n",
    "        # Reshape to use the final state from each lstm layer\n",
    "        out = ht.view(ht.size(1), self.nb_lstm_units*self.nb_layers)\n",
    "\n",
    "        # pass final states to output layer\n",
    "        out = self.hidden_to_tag(out)\n",
    "        \n",
    "        # Use logsoftmax for training and softmax for testing\n",
    "        if not self.inference:\n",
    "            Y_hat = self.logsoftmax(out)\n",
    "        else:\n",
    "            Y_hat = self.logsoftmax(out)\n",
    "\n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.8110, -2.8213, -3.0064, -2.8112, -2.8436, -2.8143, -3.0240, -2.8443,\n",
      "         -2.8831, -3.0031, -2.9265, -2.8714, -2.9578, -2.7286, -2.9425, -2.9710,\n",
      "         -2.8801, -2.9446],\n",
      "        [-2.8444, -3.0102, -2.9584, -2.8008, -2.8262, -2.9741, -2.9563, -2.8374,\n",
      "         -2.9294, -2.9198, -2.8718, -2.9824, -2.9225, -2.9095, -2.8405, -2.8606,\n",
      "         -2.7806, -2.8406],\n",
      "        [-2.8005, -2.9155, -2.7455, -2.9069, -2.9338, -2.8863, -2.9552, -2.8500,\n",
      "         -2.7989, -2.9505, -2.7973, -3.0162, -2.8965, -3.0008, -2.8984, -2.8070,\n",
      "         -2.8644, -3.0649],\n",
      "        [-2.8055, -2.9680, -2.8225, -2.8732, -2.8898, -2.8516, -2.9652, -2.8360,\n",
      "         -2.8951, -2.8889, -2.8708, -2.9137, -2.9382, -2.8891, -2.9350, -2.7997,\n",
      "         -2.8550, -3.0657],\n",
      "        [-2.8616, -2.8654, -2.8868, -2.8150, -2.9549, -2.8741, -2.9295, -2.8625,\n",
      "         -2.7780, -2.9011, -2.9353, -2.8896, -2.8693, -2.9125, -2.8961, -2.8524,\n",
      "         -3.0255, -2.9431],\n",
      "        [-2.7202, -2.9709, -2.9463, -2.9119, -2.8842, -2.8006, -3.0099, -2.8438,\n",
      "         -2.9017, -2.8492, -2.8819, -2.8306, -2.7938, -3.0109, -3.0491, -2.8930,\n",
      "         -2.9274, -2.8613],\n",
      "        [-2.9617, -3.1842, -2.8940, -2.9757, -2.8010, -2.9600, -2.9576, -2.8723,\n",
      "         -2.8405, -2.9855, -2.6869, -3.0462, -2.8975, -2.8052, -2.7901, -2.7669,\n",
      "         -2.9312, -2.7869],\n",
      "        [-2.8425, -2.9654, -2.9514, -2.8489, -2.6846, -2.8474, -2.8978, -2.9745,\n",
      "         -2.9050, -3.0521, -2.8855, -2.8226, -3.0146, -2.7856, -2.8754, -2.8695,\n",
      "         -3.0005, -2.8716]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "batch_size = 8\n",
    "lstm_unit_size = 512\n",
    "embedding_size = 512\n",
    "print_iter = len(y_train)//batch_size//6\n",
    "# print(print_iter)\n",
    "num_layers = 1\n",
    "\n",
    "m = LSTM(num_layers, char2index, class2index, nb_lstm_units = lstm_unit_size,\n",
    "         embedding_dim = embedding_size, batch_size = batch_size)\n",
    "xt_len = np.array(X_train_len[:batch_size])\n",
    "len_idx = xt_len.argsort()[::-1]\n",
    "# print(xt_len)\n",
    "# print(len_idx)\n",
    "bl = torch.tensor(xt_len[len_idx]).long()\n",
    "# print(bl)\n",
    "x = torch.tensor(np.array(X_train[:batch_size])[len_idx]).long()\n",
    "# print(x)\n",
    "print(m(x,bl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "y_train shape: (15000,)\n",
      "Iterations per epoch: 234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\Users\\chris\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185.33294677734375 \titeraton: 0 \tepoch 0 micro f1 0.09967320261437909 macro f1 0.03284978044181922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\Users\\chris\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185.33294677734375 \titeraton: 0 \tepoch 0 micro f1 0.10526315789473684 macro f1 0.04944414060811628\n",
      "117.66925048828125 \titeraton: 39 \tepoch 0 micro f1 0.4650326797385621 macro f1 0.03529265873015873\n",
      "117.66925048828125 \titeraton: 39 \tepoch 0 micro f1 0.4642502482621648 macro f1 0.03522851437398741\n",
      "109.24950408935547 \titeraton: 78 \tepoch 0 micro f1 0.46895424836601307 macro f1 0.03834103506682372\n",
      "109.24950408935547 \titeraton: 78 \tepoch 0 micro f1 0.47070506454816285 macro f1 0.039957146590950655\n",
      "133.31944274902344 \titeraton: 117 \tepoch 0 micro f1 0.47320261437908495 macro f1 0.047171766759395634\n",
      "133.31944274902344 \titeraton: 117 \tepoch 0 micro f1 0.46871896722939427 macro f1 0.04810092487769151\n",
      "117.10587310791016 \titeraton: 156 \tepoch 0 micro f1 0.4826797385620915 macro f1 0.0527449255052023\n",
      "117.10587310791016 \titeraton: 156 \tepoch 0 micro f1 0.4786494538232373 macro f1 0.05444032195657714\n",
      "109.17084503173828 \titeraton: 195 \tepoch 0 micro f1 0.49379084967320264 macro f1 0.06228715952668359\n",
      "109.17084503173828 \titeraton: 195 \tepoch 0 micro f1 0.48957298907646474 macro f1 0.06321906650775785\n",
      "110.02889251708984 \titeraton: 0 \tepoch 1 micro f1 0.48627450980392156 macro f1 0.0537538793121019\n",
      "110.02889251708984 \titeraton: 0 \tepoch 1 micro f1 0.48361469712015887 macro f1 0.05597406226758715\n",
      "95.14434814453125 \titeraton: 39 \tepoch 1 micro f1 0.48790849673202613 macro f1 0.058123371745343544\n",
      "95.14434814453125 \titeraton: 39 \tepoch 1 micro f1 0.4801390268123138 macro f1 0.0596238766801456\n",
      "94.67141723632812 \titeraton: 78 \tepoch 1 micro f1 0.4826797385620915 macro f1 0.05728886924300468\n",
      "94.67141723632812 \titeraton: 78 \tepoch 1 micro f1 0.48063555114200596 macro f1 0.0584624351985463\n",
      "107.06584930419922 \titeraton: 117 \tepoch 1 micro f1 0.47058823529411764 macro f1 0.06276723823737337\n",
      "107.06584930419922 \titeraton: 117 \tepoch 1 micro f1 0.4612711022840119 macro f1 0.06250666218883706\n",
      "87.43826293945312 \titeraton: 156 \tepoch 1 micro f1 0.49084967320261436 macro f1 0.06642996921672596\n",
      "87.43826293945312 \titeraton: 156 \tepoch 1 micro f1 0.49205561072492554 macro f1 0.06864097384424384\n",
      "106.06597137451172 \titeraton: 195 \tepoch 1 micro f1 0.49901960784313726 macro f1 0.0637307292353427\n",
      "106.06597137451172 \titeraton: 195 \tepoch 1 micro f1 0.5019860973187686 macro f1 0.06502056389099553\n",
      "101.85334777832031 \titeraton: 0 \tepoch 2 micro f1 0.47549019607843135 macro f1 0.05427645400494084\n",
      "101.85334777832031 \titeraton: 0 \tepoch 2 micro f1 0.47964250248262164 macro f1 0.05925437292891669\n",
      "106.79783630371094 \titeraton: 39 \tepoch 2 micro f1 0.4777777777777778 macro f1 0.05352839073836526\n",
      "106.79783630371094 \titeraton: 39 \tepoch 2 micro f1 0.4771598808341609 macro f1 0.05720403418481617\n",
      "99.28327178955078 \titeraton: 78 \tepoch 2 micro f1 0.48856209150326796 macro f1 0.06073294923779312\n",
      "99.28327178955078 \titeraton: 78 \tepoch 2 micro f1 0.4756703078450844 macro f1 0.06019237123131455\n",
      "83.30069732666016 \titeraton: 117 \tepoch 2 micro f1 0.4715686274509804 macro f1 0.055624433303776165\n",
      "83.30069732666016 \titeraton: 117 \tepoch 2 micro f1 0.471201588877855 macro f1 0.05961469870630116\n",
      "122.150146484375 \titeraton: 156 \tepoch 2 micro f1 0.48562091503267973 macro f1 0.05940927018038401\n",
      "122.150146484375 \titeraton: 156 \tepoch 2 micro f1 0.4791459781529295 macro f1 0.05804122449513152\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-babbba188e4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;31m#             print(m.inference)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[1;31m#batch_pred, batch_y = get_prediction(batch_x, batch_y)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             \u001b[0mbatch_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;31m#             print(batch_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;31m#             print(batch_pred)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-871fb80c0db9>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X, X_lengths)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;31m# X contains the padded sequence output and ht contains the final hidden states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;31m# for example.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mht\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mct\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;31m# Reshape to use the final state from each lstm layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[1;32m--> 182\u001b[1;33m                            self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'LSTM'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "batch_size = 64\n",
    "lstm_unit_size = 512\n",
    "embedding_size = 512\n",
    "print_iter = len(y_train)//batch_size//6\n",
    "print(print_iter)\n",
    "\n",
    "num_layers = 1\n",
    "\n",
    "m = LSTM(num_layers, char2index, class2index, nb_lstm_units = lstm_unit_size,\n",
    "         embedding_dim = embedding_size, batch_size = batch_size)\n",
    "\n",
    "criterion = nn.NLLLoss(size_average=False)\n",
    "optim = torch.optim.Adam(m.parameters(), lr=0.0001)\n",
    "\n",
    "indeces = np.arange(X_train.shape[0])     \n",
    "print('y_train shape:', y_train.shape)\n",
    "print('Iterations per epoch:', y_train.shape[0] // batch_size)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    np.random.shuffle(indeces)\n",
    "    x_train = X_train[indeces]\n",
    "#     print(x_train.shape)\n",
    "    y_train2 = y_train[indeces]\n",
    "#     print('y_train2 shape:', y_train2.shape)\n",
    "    x_lens = X_train_len[indeces]\n",
    "        \n",
    "    current_batch = 0\n",
    "    for iteration in range(y_train2.shape[0] // batch_size):\n",
    "        \n",
    "        batch_lengths = x_lens[current_batch: current_batch + batch_size]\n",
    "        lengths = np.array(batch_lengths)\n",
    "        idx = np.argsort(lengths)[::-1]\n",
    "        batch_lengths = batch_lengths[idx]\n",
    "        batch_lengths = torch.tensor(batch_lengths).long()\n",
    "        \n",
    "        \n",
    "        batch_x = X_train[current_batch: current_batch + batch_size]\n",
    "        batch_x = batch_x[idx]\n",
    "        batch_x = torch.tensor(batch_x).long()\n",
    "        \n",
    "        batch_y = y_train2[current_batch: current_batch + batch_size]\n",
    "        batch_y = batch_y[idx]\n",
    "#         print(batch_y)\n",
    "        batch_y = torch.tensor(batch_y).long()\n",
    "        \n",
    "        current_batch += batch_size\n",
    "                        \n",
    "        optim.zero_grad()\n",
    "        if len(batch_x) > 0:\n",
    "#             print(m.inference)\n",
    "            #batch_pred, batch_y = get_prediction(batch_x, batch_y)\n",
    "            batch_pred = m(batch_x, batch_lengths)\n",
    "#             print(batch_size)\n",
    "#             print(batch_pred)\n",
    "#             print(batch_y)\n",
    "            \n",
    "            loss = criterion(batch_pred, batch_y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        if iteration % print_iter == 0:\n",
    "            with torch.no_grad():\n",
    "                m.train(False)\n",
    "                m.inference = False\n",
    "                X_dev = torch.tensor(X_dev).long()\n",
    "                X_dev_len = torch.tensor(X_dev_len).long()\n",
    "                batch_pred = np.array(m(X_dev, X_dev_len)).argmax(axis=1)\n",
    "                batch_y = y_dev\n",
    "                mic_f1 = f1_score(batch_y, batch_pred, average='micro')\n",
    "                mac_f1 = f1_score(batch_y, batch_pred, average='macro')\n",
    "#                 precision = precision_score(batch_y, batch_pred, average='micro')\n",
    "#                 recall = recall_score(batch_y, batch_pred, average='micro')\n",
    "                \n",
    "                print(loss.item(), '\\titeraton:', iteration, '\\tepoch', epoch, 'micro f1', mic_f1, 'macro f1', mac_f1)\n",
    "                \n",
    "                X_test = torch.tensor(X_test).long()\n",
    "                X_test_len = torch.tensor(X_test_len).long()\n",
    "                batch_pred = np.array(m(X_test, X_test_len)).argmax(axis=1)\n",
    "                batch_y = y_test\n",
    "                f1 = f1_score(batch_y, batch_pred, average='micro')\n",
    "                mic_f1 = f1_score(batch_y, batch_pred, average='micro')\n",
    "                mac_f1 = f1_score(batch_y, batch_pred, average='macro')\n",
    "                print(loss.item(), '\\titeraton:', iteration, '\\tepoch', epoch, 'micro f1', mic_f1, 'macro f1', mac_f1)\n",
    "                m.train(True)\n",
    "                m.inference = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions below:\n",
    "\n",
    "1. What was the micro and macro F1 on the test and dev sets?\n",
    "2. Implement a bidirectional LSTM model. You will need to modify the hidden states and self.lstm variables. Does it work better?\n",
    "3. Experiments with the various hyperparameters (hidden state size, learning rate, etc.). What hyperparemeters result in the best performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART III: Natural Language Inference (25 points)\n",
    "\n",
    "Natural language inference is the task of determining whether a \"hypothesis\" is true (entailment), false (contradiction), or undetermined (neutral) given a \"premise\"[1, 2]. This task has been known to perform well for zero-shot classification[3].\n",
    "\n",
    "Example:\n",
    "\n",
    "| Premise | Label | Hypothesis |\n",
    "| ------- | ----- | ---------- |\n",
    "| A man inspects the uniform of a figure in some East Asian country. | contradiction | The man is sleeping. |\n",
    "| An older and younger man smiling | neutral | Two men are smiling and laughing at the cats playing on the floor. |\n",
    "| A soccer game with multiple males playing. | entailment | Some men are playing a sport. |\n",
    "\n",
    "Your task is to load and train a model on the \"multinli_1.0_train.jsonl\" dataset and evaluate on \"multinli_1.0_dev_matched.jsonl\" using accuracy.\n",
    "\n",
    "I am leaving this task relativley open. One solution is to modify the LSTM code above to pass two documents through a LSTM model and return the last hidden state for each. Next, concatenate the two vectors, then pass it through a softmax layer. Finally, train using the same forumlate as Part I.\n",
    "\n",
    "**NOTE:** You do not need to train until convergence. You can train for only an epoch or 2 max; train less if it takes to long. I simply want to see that it runs and is learning.\n",
    "\n",
    "\n",
    "[1] Williams, Adina, Nikita Nangia, and Samuel Bowman. \"A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference.\" Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). 2018.\n",
    "\n",
    "[2] Bowman, Samuel R., et al. \"A large annotated corpus for learning natural language inference.\" Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. 2015.\n",
    "\n",
    "[3] Yin, Wenpeng, Jamaal Hay, and Dan Roth. \"Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach.\" Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPY AND EDIT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Describe your solution.\n",
    "\n",
    "**ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA CREDIT 1 (10 points)\n",
    "\n",
    "Modify the LSTM model to train a language model, then write code to generate new text from the model. Do not forget to mask the loss function when training the language model to handle the different lengths of the sequences. Use the \"en-ud-train.upos.tsv\" dataset.\n",
    "\n",
    "Generate 10 examples from your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPY AND EDIT CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
