{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PS3: Neural Networks for Classification and Natural Language Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this task is to gain an understanding of training neural networks. Likewise, you will get to learn about the pytorch framework.\n",
    "\n",
    "## Submission Instructions\n",
    "\n",
    "After completing the exercises below, generate a pdf of the code **with** outputs. After that create a zip file containing both the completed exercise and the generated PDF. You are **required** to check the PDF to make sure all the code **and** outputs are clearly visible and easy to read. If your code goes off the page, you should reduce the line size. I generally recommend not going over 80 characters.\n",
    "\n",
    "Finally, name the zip file using a combination of your the assigment and your name, e.g., ps3_rios.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART I: Data Cleaning (10 points)\n",
    "\n",
    "Load the \"surnames.csv\" file to train a LSTM to predict nationality based on surname. You will need to transform the data from a list of strings to a list of indexes. For example, the following data\n",
    "\n",
    "```\n",
    "Anthony\n",
    "John\n",
    "David\n",
    "```\n",
    "\n",
    "should be transformed into a list of lists.\n",
    "\n",
    "```\n",
    "[[0, 1, 2, 3, 4, 1, 5],\n",
    " [6, 4, 3, 1],\n",
    " [7, 8, 9, 10, 11]]\n",
    "```\n",
    "\n",
    "Next, you will need zero-pad all examples to be the same size.\n",
    "\n",
    "```\n",
    "[[0, 1, 2, 3, 4, 1, 5],\n",
    " [6, 4, 3, 1, 0, 0, 0],\n",
    " [7, 8, 9, 10, 11, 0, 0]]\n",
    "```\n",
    "\n",
    "Finally, everything will be converted into numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_data(path):\n",
    "    ret = {'X_train':[], 'y_train':[], 'X_dev':[], 'y_dev':[], 'X_test':[], 'y_test':[], 'len':[]}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f,dialect='excel')\n",
    "        for i, row in enumerate(reader):\n",
    "#             if i > 5:\n",
    "#                 break\n",
    "#             print(row)\n",
    "            if row[0] == 'train':\n",
    "                ret['X_train'].append(row[1])\n",
    "                ret['y_train'].append(row[2])\n",
    "            if row[0] == 'test':\n",
    "                ret['X_test'].append(row[1])\n",
    "                ret['y_test'].append(row[2])\n",
    "            if row[0] == 'dev':\n",
    "                ret['X_dev'].append(row[1])\n",
    "                ret['y_dev'].append(row[2])\n",
    "            ret['len'].append(len(row[1]))\n",
    "    return ret\n",
    "    \n",
    "def load_data(path):\n",
    "    \n",
    "    results = access_data(path)\n",
    "    class2index = {}\n",
    "    for class_name in set(results['y_train']+results['y_dev']+results['y_test']):\n",
    "        class2index[class_name] = len(class2index)\n",
    "    index2class = {ind:class_name for class_name,ind in class2index.items()}\n",
    "    return results, class2index, index2class\n",
    "    \n",
    "    \n",
    "def update_mappings(X, x2index, index2x, map_elements=True):\n",
    "    if map_elements:\n",
    "        xs = set([element for x in X for element in x])\n",
    "    else:\n",
    "        xs = set([x for x in X])\n",
    "    for x in xs:\n",
    "#         print(\"\\nx:\",x)\n",
    "#         print(len(x2index))\n",
    "        x2index[x] = len(x2index)\n",
    "        index2x[len(index2x)] = x\n",
    "    \n",
    "        \n",
    "def convert_to_index_map(X, x2index, map_element=True):\n",
    "    index_mappings = []\n",
    "    for x in X:\n",
    "        if map_element:\n",
    "            index_map = [x2index[element] if element in x2index else 0 for element in x]\n",
    "        else:\n",
    "            if x in x2index:\n",
    "                index_map = x2index[x]\n",
    "            else:\n",
    "#                 print('S')\n",
    "                index_map = 0\n",
    "        index_mappings.append(index_map)\n",
    "    return index_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, 'q': 1, 'g': 2, 's': 3, 'S': 4, 'Ś': 5, 'a': 6, ',': 7, 'n': 8, 'ö': 9, 'M': 10, 'e': 11, 'ü': 12, 'I': 13, 'ñ': 14, 'U': 15, 'ą': 16, 'x': 17, 'á': 18, 'o': 19, 'à': 20, '-': 21, 'L': 22, 'z': 23, 'Á': 24, 'J': 25, 'w': 26, 'ê': 27, ' ': 28, 't': 29, 'ł': 30, 'E': 31, 'ä': 32, 'F': 33, 'H': 34, 'ß': 35, 'u': 36, 'm': 37, 'b': 38, 'ó': 39, 'X': 40, 'v': 41, 'O': 42, 'j': 43, 'i': 44, 'l': 45, 'T': 46, 'B': 47, 'í': 48, 'V': 49, 'ż': 50, 'Q': 51, 'ú': 52, 'G': 53, 'Y': 54, 'y': 55, 'k': 56, 'C': 57, '/': 58, 'ì': 59, 'é': 60, 'c': 61, 'f': 62, 'p': 63, 'h': 64, 'd': 65, 'Z': 66, 'N': 67, 'W': 68, \"'\": 69, 'A': 70, 'õ': 71, 'ã': 72, 'r': 73, 'K': 74, 'ò': 75, 'ń': 76, 'è': 77, 'ù': 78, 'D': 79, 'R': 80, 'P': 81, '1': 82}\n",
      "{'italian': 0, 'dutch': 1, 'chinese': 2, 'spanish': 3, 'japanese': 4, 'russian': 5, 'german': 6, 'french': 7, 'korean': 8, 'polish': 9, 'scottish': 10, 'czech': 11, 'greek': 12, 'portuguese': 13, 'english': 14, 'irish': 15, 'vietnamese': 16, 'arabic': 17}\n",
      "{0: 'italian', 1: 'dutch', 2: 'chinese', 3: 'spanish', 4: 'japanese', 5: 'russian', 6: 'german', 7: 'french', 8: 'korean', 9: 'polish', 10: 'scottish', 11: 'czech', 12: 'greek', 13: 'portuguese', 14: 'english', 15: 'irish', 16: 'vietnamese', 17: 'arabic'}\n"
     ]
    }
   ],
   "source": [
    "char2index = {'<PAD>': 0}\n",
    "index2char = {0: '<PAD>'}\n",
    "class2index = {} # stores the class index pairs.\n",
    "index2class = {}\n",
    "doc_lengths = [] # Stores the lengths of all docs (train, test and dev)\n",
    "X_train = [] # stores an \n",
    "y_train = [] # stores an index to the correct class\n",
    "X_dev = []\n",
    "y_dev = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "X_train_len = [] # Stores the length of each training name\n",
    "X_test_len = [] # ... length of each test name\n",
    "X_dev_len = [] # ... length of each dev name\n",
    "\n",
    "# Write code to load data here.\n",
    "dataset_filename = 'surnames.csv'\n",
    "datadir = 'data'\n",
    "dataset_path = os.path.join(datadir, dataset_filename)\n",
    "data_names = ['X_train', 'X_dev', 'X_test']\n",
    "\n",
    "\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "data, class2index, index2class = load_data(dataset_path)\n",
    "X_train, y_train, X_dev, y_dev, X_test, y_test = data['X_train'],data['y_train'],data['X_dev'],data['y_dev'] \\\n",
    "                                                 ,data['X_test'],data['y_test']\n",
    "doc_lengths = data['len']\n",
    "d = [X_train, X_dev, X_test]\n",
    "dl = [X_train_len, X_dev_len, X_test_len]\n",
    "[dl[i].append(len(x)) for i in range(len(d)) for x in d[i]]\n",
    "update_mappings(X_train, char2index, index2char)\n",
    "# print(class2index)\n",
    "# update_mappings(y_train, class2index, index2class, map_elements=False)\n",
    "print(char2index)\n",
    "print(class2index)\n",
    "print(index2class)\n",
    "\n",
    "X_train_nums, X_dev_nums = convert_to_index_map(X_train, char2index),convert_to_index_map(X_dev, char2index)\n",
    "X_test_nums = convert_to_index_map(X_test, char2index)\n",
    "y_train = convert_to_index_map(y_train, class2index, map_element=False)\n",
    "y_dev = convert_to_index_map(y_dev, class2index, map_element=False)\n",
    "y_test = convert_to_index_map(y_test, class2index, map_element=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3060\n",
      "3060\n",
      "20074\n",
      "[81, 73, 44, 56, 6, 23, 61, 64, 44, 56, 19, 41]\n",
      "Prikazchikov\n",
      "17\n",
      "Counter({5: 7050, 14: 2713, 17: 1507, 4: 770, 6: 532, 0: 526, 11: 380, 1: 223, 3: 213, 7: 203, 2: 199, 15: 174, 12: 154, 9: 103, 10: 73, 8: 66, 13: 57, 16: 57})\n",
      "tensor([0.0351, 0.0149, 0.0133, 0.0142, 0.0513, 0.4700, 0.0355, 0.0135, 0.0044,\n",
      "        0.0069, 0.0049, 0.0253, 0.0103, 0.0038, 0.1809, 0.0116, 0.0038, 0.1005])\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "print(len(X_dev))\n",
    "print(len(X_dev_len))\n",
    "print(len(doc_lengths))\n",
    "print(X_train_nums[1])\n",
    "print(X_train[1])\n",
    "print(y_train[0])\n",
    "cnt = Counter()\n",
    "cnt.update(y_train)\n",
    "print(cnt)\n",
    "cnt = [val for key, val in sorted(cnt.items(), key=lambda x: x[0])]\n",
    "class_weights = torch.FloatTensor(cnt)/sum(cnt)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longest in training set: 20\n",
      "3060\n",
      "3060\n",
      "[33 36 73 45 19  8  2  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "(15000, 20)\n"
     ]
    }
   ],
   "source": [
    "# PADDING\n",
    "\n",
    "max_seq_len = max(doc_lengths)\n",
    "len_to_pad = len(max(X_train, key=lambda x: len(x)))\n",
    "print('longest in training set:', len_to_pad)\n",
    "X_train_eq_size = []\n",
    "X_dev_eq_size = []\n",
    "X_test_eq_size = []\n",
    "\n",
    "def pad_example(ex, len_to_pad, pad):\n",
    "    padded = ex[:len_to_pad] +[pad]*(len_to_pad -len(ex))\n",
    "    return padded\n",
    "# Write code to append data to code here\n",
    "for x in X_train_nums:\n",
    "    X_train_eq_size.append(pad_example(x,len_to_pad, 0))\n",
    "    \n",
    "for x in X_dev_nums:\n",
    "    X_dev_eq_size.append(pad_example(x,len_to_pad, 0))\n",
    "    \n",
    "for x in X_test_nums:\n",
    "    X_test_eq_size.append(pad_example(x,len_to_pad, 0))\n",
    "    \n",
    "print(len(X_dev))\n",
    "X_train = np.array(X_train_eq_size)\n",
    "X_dev = np.array(X_dev_eq_size)\n",
    "X_test = np.array(X_test_eq_size)\n",
    "print(len(X_dev))\n",
    "print(X_dev[0])\n",
    "y_train = np.array(y_train)\n",
    "y_dev = np.array(y_dev)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "X_train_len = np.array(X_train_len)\n",
    "X_dev_len = np.array(X_dev_len)\n",
    "X_test_len = np.array(X_test_len)\n",
    "\n",
    "idx = np.argsort(X_dev_len)[::-1]\n",
    "X_dev = X_dev[idx]\n",
    "y_dev = y_dev[idx]\n",
    "X_dev_len = X_dev_len[idx]\n",
    "\n",
    "idx = np.argsort(X_test_len)[::-1]\n",
    "X_test = X_test[idx]\n",
    "y_test = y_test[idx]\n",
    "X_test_len = X_test_len[idx]\n",
    "\n",
    "doc_lengths = np.array(doc_lengths)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17]), array([ 526,  223,  199,  213,  770, 7050,  532,  203,   66,  103,   73,\n",
      "        380,  154,   57, 2713,  174,   57, 1507], dtype=int64))\n",
      "(3060, 20)\n",
      "(array([ 0,  1,  2,  3,  4,  5,  6,  8,  9, 10, 11, 13, 14, 15, 17, 18, 19,\n",
      "       20, 21, 22, 23, 25, 26, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39,\n",
      "       40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 51, 52, 53, 54, 55, 56, 57,\n",
      "       60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 79, 80, 81]), array([39281,     2,   333,   999,   216,     1,  2312,  1395,     1,\n",
      "         230,  1573,    35,     1,    20,     9,     1,  1625,     2,\n",
      "           4,   104,   205,   118,   131,    15,   716,    66,     1,\n",
      "          91,   178,     1,   673,   404,   330,     4,     6,   842,\n",
      "          54,    80,  1488,   880,   173,   251,     2,   117,     6,\n",
      "           2,   179,    59,   503,   870,    89,     2,   389,   199,\n",
      "         132,  1019,   465,    78,   108,    46,    15,   276,  1206,\n",
      "         169,     1,   133,   130,   154], dtype=int64))\n",
      "[[47 11 56 ... 41  0  0]\n",
      " [46 19 28 ... 11  0  0]\n",
      " [46 19 28 ...  0  0  0]\n",
      " ...\n",
      " [54 36  0 ...  0  0  0]\n",
      " [34 19  0 ...  0  0  0]\n",
      " [74 36  0 ...  0  0  0]]\n",
      "[46 19 28 46 64 11 28 33 44 73  3 29  0 28 81  6  2 11  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_train, axis=0, return_counts=True))\n",
    "print(X_dev.shape)\n",
    "print(np.unique(X_dev,  return_counts=True))\n",
    "print(X_dev)\n",
    "print(X_dev[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART II: Classification (25 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, nb_layers, word2index, class2index, nb_lstm_units=100,\n",
    "                 embedding_dim=3, batch_size=3, bidirectional=False):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.vocab = word2index\n",
    "        self.tags = class2index\n",
    "\n",
    "        self.nb_layers = nb_layers\n",
    "        self.nb_lstm_units = nb_lstm_units\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.nb_tags = len(self.tags)\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if self.bidirectional else 1\n",
    "        print('num_directions:', self.num_directions)\n",
    "        # build actual NN\n",
    "        self.__build_model()\n",
    "\n",
    "    def __build_model(self):\n",
    "        nb_vocab_words = len(self.vocab)\n",
    "\n",
    "        padding_idx = self.vocab['<PAD>']\n",
    "        self.word_embedding = nn.Embedding(\n",
    "            num_embeddings=nb_vocab_words,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.nb_lstm_units,\n",
    "            num_layers=self.nb_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            \n",
    "            self.lstm_back = nn.LSTM(\n",
    "                input_size=self.embedding_dim,\n",
    "                hidden_size=self.nb_lstm_units,\n",
    "                num_layers=self.nb_layers,\n",
    "                batch_first=True\n",
    "            )\n",
    "\n",
    "        self.hidden_to_tag = nn.Linear(self.nb_lstm_units*self.nb_layers*self.num_directions, self.nb_tags)\n",
    "        \n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.inference = False\n",
    "\n",
    "    def init_hidden(self, X, bidirectional=False):\n",
    "#         if bidirectional:\n",
    "#             h0 = torch.zeros(self.nb_layers, X.size(0), self.nb_lstm_units*self.num_directions).float()\n",
    "#             c0 = torch.zeros(self.nb_layers, X.size(0), self.nb_lstm_units*self.num_directions).float()\n",
    "#         else:\n",
    "            # Initial ht (hidden state) and ct (context)\n",
    "        h0 = torch.zeros(self.nb_layers, X.size(0), self.nb_lstm_units).float()\n",
    "        c0 = torch.zeros(self.nb_layers, X.size(0), self.nb_lstm_units).float()\n",
    "        return (h0,c0)\n",
    "\n",
    "    def forward(self, X, X_lengths):\n",
    "        # reset the LSTM hidden state. Must be done before you run a new batch.\n",
    "        # Otherwise the LSTM will treat\n",
    "        # a new batch as a continuation of a sequence\n",
    "        self.hidden = self.init_hidden(X)\n",
    "        self.hidden_back = self.init_hidden(X)\n",
    "        \n",
    "        batch_size, seq_len = X.shape\n",
    "        \n",
    "        # ---------------------\n",
    "        # 1. embed the input\n",
    "        # Dim transformation: (batch_size, seq_len) -> (batch_size, seq_len, embedding_dim)\n",
    "#         print(X[:128,:])\n",
    "#         print(X[:128,:].size())\n",
    "        \n",
    "        X = self.word_embedding(X)\n",
    "        if self.bidirectional:\n",
    "            X = torch.cat((X,torch.flip(X,[1])), 2)\n",
    "            \n",
    "\n",
    "        # ---------------------\n",
    "        # 2. Run through RNN\n",
    "        # TRICK 2 ********************************\n",
    "        # Dim transformation: (batch_size, seq_len, embedding_dim) -> (batch_size, seq_len, nb_lstm_units)\n",
    "\n",
    "    \n",
    "        # pack_padded_sequence so that padded items in the sequence won't be shown to the LSTM\n",
    "        if self.bidirectional:\n",
    "            X_for = torch.nn.utils.rnn.pack_padded_sequence(X[:,:,:self.embedding_dim], X_lengths, batch_first=True)\n",
    "            X_back = torch.nn.utils.rnn.pack_padded_sequence(X[:,:,self.embedding_dim:], X_lengths, batch_first=True)\n",
    "        else:\n",
    "            X = torch.nn.utils.rnn.pack_padded_sequence(X, X_lengths, batch_first=True)\n",
    "        \n",
    "        \n",
    "        # now run through LSTM\n",
    "        # X contains the padded sequence output and ht contains the final hidden states\n",
    "        if self.bidirectional:\n",
    "            X_for, (ht_for, ct_for) = self.lstm(X_for, self.hidden)\n",
    "#             X_back, (ht_back, ct_back) = self.lstm(X_back, self.hidden_back)\n",
    "            X_back, (ht_back, ct_back) = self.lstm_back(X_back, self.hidden) \n",
    "            ht = torch.cat((ht_for, ht_back), 2)\n",
    "        else:\n",
    "            X, (ht, ct) = self.lstm(X, self.hidden)\n",
    "        \n",
    "#         print('hidden state shape:', ht.size())\n",
    "        \n",
    "        # Reshape to use the final state from each lstm layer\n",
    "        out = ht.view(ht.size(1), self.nb_lstm_units*self.nb_layers*self.num_directions)\n",
    "\n",
    "        # pass final states to output layer\n",
    "        out = self.hidden_to_tag(out)\n",
    "        \n",
    "        # Use logsoftmax for training and softmax for testing\n",
    "        if not self.inference:\n",
    "            Y_hat = self.logsoftmax(out)\n",
    "        else:\n",
    "            Y_hat = self.softmax(out)\n",
    "\n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_directions: 1\n",
      "y_train shape: (15000,)\n",
      "Iterations per epoch: 937\n",
      "Number of prints per epoch: 12\n",
      "\ttraining loss 2.932\titeraton: 0\tepoch 0 \n",
      "\tTrain:\n",
      "\t\tmicro f1 0.812 macro f1 0.682\n",
      "\tDev:\n",
      "\t\tmicro f1 0.411 macro f1 0.053\n",
      "\n",
      "\ttraining loss 1.148\titeraton: 78\tepoch 0 \n",
      "\tTrain:\n",
      "\t\tmicro f1 0.812 macro f1 0.764\n",
      "\tDev:\n",
      "\t\tmicro f1 0.651 macro f1 0.230\n",
      "\n",
      "\ttraining loss 0.966\titeraton: 156\tepoch 0 \n",
      "\tTrain:\n",
      "\t\tmicro f1 0.750 macro f1 0.438\n",
      "\tDev:\n",
      "\t\tmicro f1 0.702 macro f1 0.267\n",
      "\n",
      "\ttraining loss 1.787\titeraton: 234\tepoch 0 \n",
      "\tTrain:\n",
      "\t\tmicro f1 0.438 macro f1 0.152\n",
      "\tDev:\n",
      "\t\tmicro f1 0.715 macro f1 0.280\n",
      "\n",
      "\ttraining loss 0.947\titeraton: 312\tepoch 0 \n",
      "\tTrain:\n",
      "\t\tmicro f1 0.812 macro f1 0.747\n",
      "\tDev:\n",
      "\t\tmicro f1 0.722 macro f1 0.298\n",
      "\n",
      "\ttraining loss 1.689\titeraton: 390\tepoch 0 \n",
      "\tTrain:\n",
      "\t\tmicro f1 0.500 macro f1 0.216\n",
      "\tDev:\n",
      "\t\tmicro f1 0.737 macro f1 0.283\n",
      "\n",
      "\ttraining loss 1.029\titeraton: 468\tepoch 0 \n",
      "\tTrain:\n",
      "\t\tmicro f1 0.688 macro f1 0.318\n",
      "\tDev:\n",
      "\t\tmicro f1 0.730 macro f1 0.297\n",
      "\n",
      "\ttraining loss 0.626\titeraton: 546\tepoch 0 \n",
      "\tTrain:\n",
      "\t\tmicro f1 0.812 macro f1 0.437\n",
      "\tDev:\n",
      "\t\tmicro f1 0.740 macro f1 0.342\n",
      "\n",
      "\ttraining loss 1.166\titeraton: 624\tepoch 0 \n",
      "\tTrain:\n",
      "\t\tmicro f1 0.688 macro f1 0.296\n",
      "\tDev:\n",
      "\t\tmicro f1 0.735 macro f1 0.313\n",
      "\n",
      "\ttraining loss 0.758\titeraton: 702\tepoch 0 \n",
      "\tTrain:\n",
      "\t\tmicro f1 0.812 macro f1 0.804\n",
      "\tDev:\n",
      "\t\tmicro f1 0.727 macro f1 0.310\n",
      "\n",
      "\ttraining loss 0.469\titeraton: 780\tepoch 0 \n",
      "\tTrain:\n",
      "\t\tmicro f1 0.875 macro f1 0.726\n",
      "\tDev:\n",
      "\t\tmicro f1 0.742 macro f1 0.331\n",
      "\n",
      "\ttraining loss 0.563\titeraton: 858\tepoch 0 \n",
      "\tTrain:\n",
      "\t\tmicro f1 0.812 macro f1 0.678\n",
      "\tDev:\n",
      "\t\tmicro f1 0.739 macro f1 0.325\n",
      "\n",
      "\ttraining loss 0.495\titeraton: 936\tepoch 0 \n",
      "\tTrain:\n",
      "\t\tmicro f1 0.750 macro f1 0.344\n",
      "\tDev:\n",
      "\t\tmicro f1 0.729 macro f1 0.308\n",
      "\n",
      "[5 5 5 5 5 5 5 5 5 5]\n",
      "[ 7  5  2  5  5  5 11  5  0 14]\n",
      "EPOCH SUMMARY:\n",
      "training loss 4.388\titeraton: 936\tepoch 0 \n",
      "Train:\n",
      "\tmicro f1 0.293 macro f1 0.058\n",
      "Dev:\n",
      "\tmicro f1 0.350 macro f1 0.079\n",
      "\n",
      "\ttraining loss 0.476\titeraton: 0\tepoch 1 \n",
      "\tTrain:\n",
      "\t\tmicro f1 0.875 macro f1 0.489\n",
      "\tDev:\n",
      "\t\tmicro f1 0.731 macro f1 0.307\n",
      "\n",
      "\ttraining loss 1.107\titeraton: 78\tepoch 1 \n",
      "\tTrain:\n",
      "\t\tmicro f1 0.688 macro f1 0.465\n",
      "\tDev:\n",
      "\t\tmicro f1 0.742 macro f1 0.318\n",
      "\n",
      "\ttraining loss 0.665\titeraton: 156\tepoch 1 \n",
      "\tTrain:\n",
      "\t\tmicro f1 0.875 macro f1 0.667\n",
      "\tDev:\n",
      "\t\tmicro f1 0.750 macro f1 0.330\n",
      "\n",
      "\ttraining loss 1.416\titeraton: 234\tepoch 1 \n",
      "\tTrain:\n",
      "\t\tmicro f1 0.562 macro f1 0.239\n",
      "\tDev:\n",
      "\t\tmicro f1 0.753 macro f1 0.346\n",
      "\n",
      "\ttraining loss 0.695\titeraton: 312\tepoch 1 \n",
      "\tTrain:\n",
      "\t\tmicro f1 0.812 macro f1 0.639\n",
      "\tDev:\n",
      "\t\tmicro f1 0.739 macro f1 0.320\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-3656dde6bd67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[0mtrain_mac_f1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_batch_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'macro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m                 \u001b[0mdev_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp_X_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp_X_dev_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m                 \u001b[1;31m#There is a bug in the shuffling here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-97434e0dcbdf>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X, X_lengths)\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[0mht\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mht_for\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mht_back\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mht\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mct\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;31m#         print('hidden state shape:', ht.size())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[1;32m--> 182\u001b[1;33m                            self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'LSTM'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_layers = 1\n",
    "epochs = 15\n",
    "batch_size = 16\n",
    "lstm_unit_size = 128\n",
    "embedding_size = 128\n",
    "prints_per_epoch = 12\n",
    "print_iter = len(y_train)//batch_size//prints_per_epoch\n",
    "bidirectional = False\n",
    "\n",
    "m = LSTM(num_layers, char2index, class2index, nb_lstm_units = lstm_unit_size,\n",
    "         embedding_dim = embedding_size, batch_size = batch_size, bidirectional=bidirectional)\n",
    "\n",
    "criterion = nn.NLLLoss()#size_average=False,weight=1/class_weights\n",
    "optim = torch.optim.Adam(m.parameters(), lr=0.01)\n",
    "\n",
    "indeces = np.arange(X_train.shape[0])     \n",
    "print('y_train shape:', y_train.shape)\n",
    "print('Iterations per epoch:', y_train.shape[0] // batch_size)\n",
    "print('Number of prints per epoch:',y_train.shape[0] //batch_size // print_iter)\n",
    "\n",
    "dev_idx = np.argsort(np.array(X_dev_len))[::-1]\n",
    "np_X_dev = torch.tensor(X_dev[dev_idx]).long()\n",
    "np_X_dev_len = torch.tensor(X_dev_len[dev_idx]).long() \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    np.random.shuffle(indeces)\n",
    "    x_train = X_train[indeces]\n",
    "    y_train2 = y_train[indeces]\n",
    "    x_lens = X_train_len[indeces]\n",
    "\n",
    "    np_x_sorted_lens = np.array(x_lens)[np.argsort(np.array(x_lens))[::-1]]\n",
    "    current_batch = 0\n",
    "    for iteration in range(y_train2.shape[0] // batch_size):\n",
    "        \n",
    "        batch_lengths = x_lens[current_batch: current_batch + batch_size]\n",
    "        lengths = np.array(batch_lengths)\n",
    "        idx = np.argsort(lengths)[::-1]\n",
    "        batch_lengths = batch_lengths[idx]\n",
    "        batch_lengths = torch.tensor(batch_lengths).long()\n",
    "        \n",
    "        \n",
    "        batch_x = x_train[current_batch: current_batch + batch_size]\n",
    "        batch_x = batch_x[idx]\n",
    "        batch_x = torch.tensor(batch_x).long()\n",
    "        \n",
    "        batch_y = y_train2[current_batch: current_batch + batch_size]\n",
    "        batch_y = batch_y[idx]\n",
    "        batch_y = torch.tensor(batch_y).long()\n",
    "        \n",
    "        current_batch += batch_size\n",
    "                        \n",
    "        optim.zero_grad()\n",
    "        if len(batch_x) > 0:\n",
    "            batch_pred = m(batch_x, batch_lengths)\n",
    "            \n",
    "            loss = criterion(batch_pred, batch_y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        if iteration % print_iter == 0:\n",
    "            with torch.no_grad():\n",
    "                m.train(False)\n",
    "                m.inference = True\n",
    "                train_batch_pred = np.array(m(batch_x, batch_lengths)).argmax(axis=1)\n",
    "                train_mic_f1 = f1_score(batch_y, train_batch_pred, average='micro')\n",
    "                train_mac_f1 = f1_score(batch_y, train_batch_pred, average='macro')\n",
    "          \n",
    "                dev_pred = np.array(m(np_X_dev, np_X_dev_len)).argmax(axis=1)\n",
    "                \n",
    "                #There is a bug in the shuffling here\n",
    "                dev_batch_y = y_dev[dev_idx]\n",
    "                dev_mic_f1 = f1_score(dev_batch_y, dev_pred, average='micro')\n",
    "                dev_mac_f1 = f1_score(dev_batch_y, dev_pred, average='macro')\n",
    "#                 precision = precision_score(batch_y, batch_pred, average='micro')\n",
    "#                 recall = recall_score(batch_y, batch_pred, average='micro')\n",
    "                print(f'\\ttraining loss {loss.item():.3f}\\titeraton: { iteration}\\tepoch {epoch} ')\n",
    "                print('\\tTrain:')      \n",
    "                print(f'\\t\\tmicro f1 { train_mic_f1:.3f} macro f1 {train_mac_f1:.3f}')\n",
    "                print('\\tDev:')      \n",
    "                print(f'\\t\\tmicro f1 { dev_mic_f1:.3f} macro f1 {dev_mac_f1:.3f}\\n')\n",
    "#                 print('\\tPrediction counts:')\n",
    "#                 uniques = np.unique(train_batch_pred, axis=0, return_counts=True)\n",
    "#                 print('\\tpred indeces')\n",
    "#                 print(uniques[0])\n",
    "#                 print(uniques[0][0])\n",
    "#                 print(len(uniques[0]))\n",
    "#                 print('\\t\\t',end='')\n",
    "#                 [print(str(uniques[0][i])+'  ', end='') for i in range(len(uniques[0]))]\n",
    "#                 print()\n",
    "#                 print('\\tpred counts')\n",
    "#                 print('\\t\\t',end='')\n",
    "#                 [print(str(uniques[1][i])+'  ', end='') for i in range(len(uniques[0]))]\n",
    "#                 print()\n",
    "#                 print('\\ttrue indeces')\n",
    "#                 print('\\t\\t',end='')\n",
    "#                 true_unqs = np.unique(batch_y, axis=0, return_counts=True)\n",
    "#                 [print(str(true_unqs[0][i])+'  ', end='') for i in range(len(true_unqs[0]))]\n",
    "#                 print()\n",
    "#                 print('\\ttrue counts')\n",
    "#                 print('\\t\\t',end='')\n",
    "#                 [print(str(true_unqs[1][i])+'  ', end='') for i in range(len(true_unqs[0]))]\n",
    "#                 print()\n",
    "#                 \n",
    "                m.train(True)\n",
    "                m.inference = False\n",
    "        \n",
    "        \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        m.train(False)\n",
    "        m.inference = False\n",
    "        np_full_lens = np.array(x_lens\n",
    "        full_idx = np.argsort()[::-1]\n",
    "        raw_pred = m(torch.tensor(x_train[full_idx]).long(),torch.tensor(x_lens[full_idx]).long() )\n",
    "        train_pred = np.array(raw_pred).argmax(axis=1)\n",
    "        train_mic_f1 = f1_score(y_train2, train_pred, average='micro')\n",
    "        print(train_pred[:10])\n",
    "        print(y_train2[:10])\n",
    "        train_mac_f1 = f1_score(y_train2, train_pred, average='macro')\n",
    "        loss = criterion(raw_pred, torch.tensor(y_train2).long())\n",
    "\n",
    "        \n",
    "        dev_pred = np.array(m(np_X_dev, np_X_dev_len)).argmax(axis=1)\n",
    "\n",
    "        dev_batch_y = y_dev[dev_idx]\n",
    "        dev_mic_f1 = f1_score(y_dev, dev_pred, average='micro')\n",
    "        dev_mac_f1 = f1_score(y_dev, dev_pred, average='macro')\n",
    "#                 precision = precision_score(batch_y, batch_pred, average='micro')\n",
    "#                 recall = recall_score(batch_y, batch_pred, average='micro')\n",
    "        print('EPOCH SUMMARY:')\n",
    "        print(f'training loss {loss.item():.3f}\\titeraton: { iteration}\\tepoch {epoch} ')\n",
    "        print('Train:')      \n",
    "        print(f'\\tmicro f1 { train_mic_f1:.3f} macro f1 {train_mac_f1:.3f}')\n",
    "        print('Dev:')      \n",
    "        print(f'\\tmicro f1 { dev_mic_f1:.3f} macro f1 {dev_mac_f1:.3f}\\n')\n",
    "#                 print('\\tPrediction counts:')\n",
    "#                 uniques = np.unique(train_batch_pred, axis=0, return_counts=True)\n",
    "#                 print('\\tpred indeces')\n",
    "#                 print(uniques[0])\n",
    "#                 print(uniques[0][0])\n",
    "#                 print(len(uniques[0]))\n",
    "#                 print('\\t\\t',end='')\n",
    "#                 [print(str(uniques[0][i])+'  ', end='') for i in range(len(uniques[0]))]\n",
    "#                 print()\n",
    "#                 print('\\tpred counts')\n",
    "#                 print('\\t\\t',end='')\n",
    "#                 [print(str(uniques[1][i])+'  ', end='') for i in range(len(uniques[0]))]\n",
    "#                 print()\n",
    "#                 print('\\ttrue indeces')\n",
    "#                 print('\\t\\t',end='')\n",
    "#                 true_unqs = np.unique(batch_y, axis=0, return_counts=True)\n",
    "#                 [print(str(true_unqs[0][i])+'  ', end='') for i in range(len(true_unqs[0]))]\n",
    "#                 print()\n",
    "#                 print('\\ttrue counts')\n",
    "#                 print('\\t\\t',end='')\n",
    "#                 [print(str(true_unqs[1][i])+'  ', end='') for i in range(len(true_unqs[0]))]\n",
    "#                 print()\n",
    "#                 \n",
    "        m.train(True)\n",
    "        m.inference = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions below:\n",
    "\n",
    "1. What was the micro and macro F1 on the test and dev sets?\n",
    "\n",
    "They have a lot of variation, but both hover around the high 40s for micro, then in the teens for the macro f1\n",
    "2. Implement a bidirectional LSTM model. You will need to modify the hidden states and self.lstm variables. Does it work better? \n",
    "No, it didn't\n",
    "3. Experiments with the various hyperparameters (hidden state size, learning rate, etc.). What hyperparemeters result in the best performance? \n",
    "Many of them seemed similar. The original ones were among the best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART III: Natural Language Inference (25 points)\n",
    "\n",
    "Natural language inference is the task of determining whether a \"hypothesis\" is true (entailment), false (contradiction), or undetermined (neutral) given a \"premise\"[1, 2]. This task has been known to perform well for zero-shot classification[3].\n",
    "\n",
    "Example:\n",
    "\n",
    "| Premise | Label | Hypothesis |\n",
    "| ------- | ----- | ---------- |\n",
    "| A man inspects the uniform of a figure in some East Asian country. | contradiction | The man is sleeping. |\n",
    "| An older and younger man smiling | neutral | Two men are smiling and laughing at the cats playing on the floor. |\n",
    "| A soccer game with multiple males playing. | entailment | Some men are playing a sport. |\n",
    "\n",
    "Your task is to load and train a model on the \"multinli_1.0_train.jsonl\" dataset and evaluate on \"multinli_1.0_dev_matched.jsonl\" using accuracy.\n",
    "\n",
    "I am leaving this task relativley open. One solution is to modify the LSTM code above to pass two documents through a LSTM model and return the last hidden state for each. Next, concatenate the two vectors, then pass it through a softmax layer. Finally, train using the same forumlate as Part I.\n",
    "\n",
    "**NOTE:** You do not need to train until convergence. You can train for only an epoch or 2 max; train less if it takes to long. I simply want to see that it runs and is learning.\n",
    "\n",
    "\n",
    "[1] Williams, Adina, Nikita Nangia, and Samuel Bowman. \"A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference.\" Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). 2018.\n",
    "\n",
    "[2] Bowman, Samuel R., et al. \"A large annotated corpus for learning natural language inference.\" Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. 2015.\n",
    "\n",
    "[3] Yin, Wenpeng, Jamaal Hay, and Dan Roth. \"Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach.\" Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPY AND EDIT CODE HERE\n",
    "import json\n",
    "\n",
    "train_path = 'data/multinli_1.0_train.jsonl'\n",
    "\n",
    "def load_data(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = f.readlines()\n",
    "        \n",
    "    X = []\n",
    "    Y = []\n",
    "    for d in data:\n",
    "    #     print(d)\n",
    "        d_dict = json.loads(d)\n",
    "    #     print(d_dict)\n",
    "    #     print(type(d_dict))\n",
    "        X.append((d_dict['sentence1'], d_dict['sentence2']))\n",
    "        Y.append(d_dict['gold_label'])\n",
    "    return X, Y\n",
    "\n",
    "trainX, trainY = load_data(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# This takes like 5-10 minutes to run\n",
    "# flipped = [a for a in zip(*trainX)]\n",
    "trainX = [[word_tokenize(sent) for sent in a] for a in trainX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401\n"
     ]
    }
   ],
   "source": [
    "trainX_lens = [ [len(t) for t in tup] for tup in  trainX]\n",
    "max_len = max([ max(a) for a in zip(*trainX_lens)])\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392702\n"
     ]
    }
   ],
   "source": [
    "print(len(trainX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_example(ex, len_to_pad, pad):\n",
    "    padded = [ex[0][:len_to_pad] +[pad]*(len_to_pad -len(ex[0])),ex[1][:len_to_pad] +[pad]*(len_to_pad -len(ex[1]))]\n",
    "    return padded\n",
    "# Write code to append data to code here\n",
    "# for x in X_train_nums:\n",
    "#     X_train_eq_size.append(pad_example(x,max_len, 0))\n",
    "    \n",
    "# for x in X_dev_nums:\n",
    "#     X_dev_eq_size.append(pad_example(x,max_len, 0))\n",
    "    \n",
    "# for x in X_test_nums:\n",
    "#     X_test_eq_size.append(pad_example(x,max_len, 0))\n",
    "\n",
    "def indecize(sent, tok2index):\n",
    "    indeces = []\n",
    "    for tok in sent:\n",
    "        if tok in tok2index:\n",
    "            indeces.append(tok2index[tok])\n",
    "        else:\n",
    "            tok2index[tok] = len(tok2index)\n",
    "            indeces.append(tok2index[tok])\n",
    "    return indeces\n",
    "    \n",
    "def indecize_trainX(train_toked, tok2index):\n",
    "    train_indeces = []\n",
    "    for tup in train_toked:\n",
    "        train_row = []\n",
    "        for t in tup:\n",
    "            train_row.append(indecize(t, tok2index))\n",
    "        train_indeces.append(train_row)\n",
    "    return train_indeces\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "tok2index = {'<PAD>': 0}\n",
    "index2tok = {0: '<PAD>'}\n",
    "class2index = {} # stores the class index pairs.\n",
    "index2class = {}\n",
    "# train_toked = [t for t in zip(*train_toked)]\n",
    "trainX_nums = indecize_trainX(trainX, tok2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392702\n"
     ]
    }
   ],
   "source": [
    "print(len(trainX_nums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392702\n",
      "101901\n"
     ]
    }
   ],
   "source": [
    "# print(t[0][1])\n",
    "# trainX_nums = [x for x in zip(*trainX_nums)]\n",
    "print(len(trainX_nums))\n",
    "print(len(tok2index))\n",
    "trainX_eq_size = []\n",
    "# Write code to append data to code here\n",
    "for x in trainX_nums:\n",
    "#     print(len(x[0]), len(x[1]))\n",
    "    new_x = pad_example(x,max_len, 0)\n",
    "#     print(len(new_x[0]), len(new_x[1]))\n",
    "    trainX_eq_size.append(pad_example(x,max_len, 0))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a, b in trainX_eq_size:\n",
    "#     if len(a) != max_len or len(b) != max_len:\n",
    "#         print('ding!', len(a), len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392702\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, nb_layers, word2index, class2index, nb_lstm_units=100,\n",
    "                 embedding_dim=3, batch_size=3, bidirectional=False):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.vocab = word2index\n",
    "        self.tags = class2index\n",
    "\n",
    "        self.nb_layers = nb_layers\n",
    "        self.nb_lstm_units = nb_lstm_units\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.nb_tags = len(self.tags)\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if self.bidirectional else 1\n",
    "        print('num_directions:', self.num_directions)\n",
    "        # build actual NN\n",
    "        self.__build_model()\n",
    "\n",
    "    def __build_model(self):\n",
    "        nb_vocab_words = len(self.vocab)\n",
    "\n",
    "        padding_idx = self.vocab['<PAD>']\n",
    "        self.word_embedding = nn.Embedding(\n",
    "            num_embeddings=nb_vocab_words,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.nb_lstm_units,\n",
    "            num_layers=self.nb_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            \n",
    "            self.lstm_back = nn.LSTM(\n",
    "                input_size=self.embedding_dim,\n",
    "                hidden_size=self.nb_lstm_units,\n",
    "                num_layers=self.nb_layers,\n",
    "                batch_first=True\n",
    "            )\n",
    "\n",
    "        self.hidden_to_tag = nn.Linear(self.nb_lstm_units*self.nb_layers*self.num_directions*2, self.nb_tags)\n",
    "        \n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.inference = False\n",
    "\n",
    "    def init_hidden(self, X, bidirectional=False):\n",
    "#         if bidirectional:\n",
    "#             h0 = torch.zeros(self.nb_layers, X.size(0), self.nb_lstm_units*self.num_directions).float()\n",
    "#             c0 = torch.zeros(self.nb_layers, X.size(0), self.nb_lstm_units*self.num_directions).float()\n",
    "#         else:\n",
    "            # Initial ht (hidden state) and ct (context)\n",
    "        h0 = torch.zeros(self.nb_layers, X.size(0), self.nb_lstm_units).float()\n",
    "        c0 = torch.zeros(self.nb_layers, X.size(0), self.nb_lstm_units).float()\n",
    "        return (h0,c0)\n",
    "    \n",
    "    def embed_sent(self, X, X_lengths):\n",
    "        self.hidden = self.init_hidden(X)\n",
    "        self.hidden_back = self.init_hidden(X)\n",
    "        \n",
    "        batch_size, seq_len = X.shape\n",
    "        \n",
    "        # ---------------------\n",
    "        # 1. embed the input\n",
    "        # Dim transformation: (batch_size, seq_len) -> (batch_size, seq_len, embedding_dim)\n",
    "#         print(X[:128,:])\n",
    "#         print(X[:128,:].size())\n",
    "        \n",
    "        X = self.word_embedding(X)\n",
    "        if self.bidirectional:\n",
    "            X = torch.cat((X,torch.flip(X,[1])), 2)\n",
    "            \n",
    "\n",
    "        # ---------------------\n",
    "        # 2. Run through RNN\n",
    "        # TRICK 2 ********************************\n",
    "        # Dim transformation: (batch_size, seq_len, embedding_dim) -> (batch_size, seq_len, nb_lstm_units)\n",
    "\n",
    "    \n",
    "        # pack_padded_sequence so that padded items in the sequence won't be shown to the LSTM\n",
    "        if self.bidirectional:\n",
    "            X_for = torch.nn.utils.rnn.pack_padded_sequence(X[:,:,:self.embedding_dim], X_lengths, batch_first=True)\n",
    "            X_back = torch.nn.utils.rnn.pack_padded_sequence(X[:,:,self.embedding_dim:], X_lengths, batch_first=True)\n",
    "        else:\n",
    "            X = torch.nn.utils.rnn.pack_padded_sequence(X, X_lengths, batch_first=True)\n",
    "        \n",
    "        \n",
    "        # now run through LSTM\n",
    "        # X contains the padded sequence output and ht contains the final hidden states\n",
    "        if self.bidirectional:\n",
    "            X_for, (ht_for, ct_for) = self.lstm(X_for, self.hidden)\n",
    "            X_back, (ht_back, ct_back) = self.lstm(X_back, self.hidden_back)\n",
    "#             X_back, (ht_back, ct_back) = self.lstm_back(X_back, self.hidden) \n",
    "            ht = torch.cat((ht_for, ht_back), 2)\n",
    "        else:\n",
    "            X, (ht, ct) = self.lstm(X, self.hidden)\n",
    "        \n",
    "#         print('hidden state shape:', ht.size())\n",
    "        \n",
    "        # Reshape to use the final state from each lstm layer\n",
    "        out = ht.view(ht.size(1), self.nb_lstm_units*self.nb_layers*self.num_directions)\n",
    "        return out\n",
    "        \n",
    "    def forward(self, X, X_lengths):\n",
    "        # reset the LSTM hidden state. Must be done before you run a new batch.\n",
    "        # Otherwise the LSTM will treat\n",
    "        # a new batch as a continuation of a sequence\n",
    "        column0 = embed_sent(X[0], X_lengths[0])\n",
    "        column1 = embed_sent(X[1], X_lengths[1])\n",
    "        tot_col = np.concatenate((column0, column1), axis=1)\n",
    "\n",
    "        # pass final states to output layer\n",
    "        \n",
    "        out = self.hidden_to_tag(tot_col)\n",
    "        \n",
    "        # Use logsoftmax for training and softmax for testing\n",
    "        if not self.inference:\n",
    "            Y_hat = self.logsoftmax(out)\n",
    "        else:\n",
    "            Y_hat = self.softmax(out)\n",
    "\n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392702\n",
      "[0, 1, 1, 1, 0, 1, 0, 1, 2, 2, 1, 2, 0, 0, 2, 1, 1, 2, 2, 0, 1, 1, 2, 1, 1, 0, 2, 0, 2, 1, 1, 1, 2, 0, 2, 1, 1, 2, 2, 2, 1, 0, 1, 0, 1, 1, 2, 2, 0, 1, 1, 1, 2, 0, 0, 0, 2, 0, 2, 0, 0, 1, 2, 1, 0, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 1, 2, 0, 1, 0, 2, 2, 2, 1, 0, 1, 0, 1, 2, 0, 0, 2, 2, 1, 1, 2, 0, 1, 2, 1]\n",
      "{'neutral': 0, 'entailment': 1, 'contradiction': 2}\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "class2index = {}\n",
    "print(len(trainX_eq_size))\n",
    "for class_type in trainY:\n",
    "    if class_type not in class2index:\n",
    "        class2index[class_type] = len(class2index)\n",
    "train_Y = convert_to_index_map(trainY, class2index, map_element=False)\n",
    "print(train_Y[:100])\n",
    "print(class2index)\n",
    "train_Y = np.array(train_Y)\n",
    "print(train_Y[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(392702, 2, 401)\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "(392702, 2, 401)\n"
     ]
    }
   ],
   "source": [
    "train_X = np.array(trainX_eq_size)\n",
    "print(train_X.shape)\n",
    "print(train_X[0][0])\n",
    "for ex in trainX_eq_size[:1000]:\n",
    "    for i, sent in enumerate(ex):\n",
    "        if len(sent) != 401:\n",
    "            print(len(sent), i)\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 1\n",
    "epochs = 15\n",
    "batch_size = 16\n",
    "lstm_unit_size = 64\n",
    "embedding_size = 64\n",
    "prints_per_epoch = 12\n",
    "\n",
    "\n",
    "\n",
    "print_iter = len(train_Y)//batch_size//prints_per_epoch\n",
    "bidirectional = False\n",
    "\n",
    "m = LSTM(num_layers, tok2index, class2index, nb_lstm_units = lstm_unit_size,\n",
    "         embedding_dim = embedding_size, batch_size = batch_size, bidirectional=bidirectional)\n",
    "\n",
    "criterion = nn.NLLLoss() #size_average=False,weight=1/class_weights\n",
    "optim = torch.optim.Adam(m.parameters(), lr=0.01)\n",
    "\n",
    "indeces = np.arange(train_X.shape[0])     \n",
    "print('y_train shape:', train_Y.shape)\n",
    "print('Iterations per epoch:', train_Y.shape[0] // batch_size)\n",
    "print('Number of prints per epoch:',print_iter)\n",
    "\n",
    "# np_X_dev = torch.tensor(X_dev).long()\n",
    "# np_X_dev_len = torch.tensor(np.array(X_dev_len)[np.argsort(np.array(X_dev_len))[::-1]] ).long()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    np.random.shuffle(indeces)\n",
    "    train_x = X_train[indeces]\n",
    "    train_y = train_Y[indeces]\n",
    "    lens_x = trainX_lens[indeces]\n",
    "\n",
    "    np_x_sorted_lens = np.array(lens_x)[np.argsort(np.array(lens_x))[::-1]]\n",
    "    current_batch = 0\n",
    "    for iteration in range(y_train2.shape[0] // batch_size):\n",
    "        \n",
    "        batch_lengths = lens_x[current_batch: current_batch + batch_size]\n",
    "        lengths = np.array(batch_lengths)\n",
    "        idx = np.argsort(lengths)[::-1]\n",
    "        batch_lengths = batch_lengths[idx]\n",
    "        batch_lengths = torch.tensor(batch_lengths).long()\n",
    "        \n",
    "        \n",
    "        batch_x = train_x[current_batch: current_batch + batch_size]\n",
    "        batch_x = batch_x[idx]\n",
    "        batch_x = torch.tensor(batch_x).long()\n",
    "        \n",
    "        batch_y = y_train2[current_batch: current_batch + batch_size]\n",
    "        batch_y = batch_y[idx]\n",
    "        batch_y = torch.tensor(batch_y).long()\n",
    "        \n",
    "        current_batch += batch_size\n",
    "                        \n",
    "        optim.zero_grad()\n",
    "        if len(batch_x) > 0:\n",
    "            batch_pred = m(batch_x, batch_lengths)\n",
    "            \n",
    "            loss = criterion(batch_pred, batch_y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        if iteration % print_iter == 0:\n",
    "            with torch.no_grad():\n",
    "                m.train(False)\n",
    "                m.inference = True\n",
    "                train_batch_pred = np.array(m(batch_x, batch_lengths)).argmax(axis=1)\n",
    "                train_mic_f1 = f1_score(batch_y, train_batch_pred, average='micro')\n",
    "                train_mac_f1 = f1_score(batch_y, train_batch_pred, average='macro')\n",
    "          \n",
    "                batch_pred = np.array(m(np_X_dev, np_X_dev_len)).argmax(axis=1)\n",
    "                \n",
    "                dev_batch_y = y_dev\n",
    "                dev_mic_f1 = f1_score(dev_batch_y, batch_pred, average='micro')\n",
    "                dev_mac_f1 = f1_score(dev_batch_y, batch_pred, average='macro')\n",
    "#                 precision = precision_score(batch_y, batch_pred, average='micro')\n",
    "#                 recall = recall_score(batch_y, batch_pred, average='micro')\n",
    "                print(f'\\ttraining loss {loss.item():.3f}\\titeraton: { iteration}\\tepoch {epoch} ')\n",
    "                print('\\tTrain:')      \n",
    "                print(f'\\t\\tmicro f1 { train_mic_f1:.3f} macro f1 {train_mac_f1:.3f}')\n",
    "                print('\\tDev:')      \n",
    "                print(f'\\t\\tmicro f1 { dev_mic_f1:.3f} macro f1 {dev_mac_f1:.3f}\\n')\n",
    "#                 print('\\tPrediction counts:')\n",
    "#                 uniques = np.unique(train_batch_pred, axis=0, return_counts=True)\n",
    "#                 print('\\tpred indeces')\n",
    "#                 print(uniques[0])\n",
    "#                 print(uniques[0][0])\n",
    "#                 print(len(uniques[0]))\n",
    "#                 print('\\t\\t',end='')\n",
    "#                 [print(str(uniques[0][i])+'  ', end='') for i in range(len(uniques[0]))]\n",
    "#                 print()\n",
    "#                 print('\\tpred counts')\n",
    "#                 print('\\t\\t',end='')\n",
    "#                 [print(str(uniques[1][i])+'  ', end='') for i in range(len(uniques[0]))]\n",
    "#                 print()\n",
    "#                 print('\\ttrue indeces')\n",
    "#                 print('\\t\\t',end='')\n",
    "#                 true_unqs = np.unique(batch_y, axis=0, return_counts=True)\n",
    "#                 [print(str(true_unqs[0][i])+'  ', end='') for i in range(len(true_unqs[0]))]\n",
    "#                 print()\n",
    "#                 print('\\ttrue counts')\n",
    "#                 print('\\t\\t',end='')\n",
    "#                 [print(str(true_unqs[1][i])+'  ', end='') for i in range(len(true_unqs[0]))]\n",
    "#                 print()\n",
    "#                 \n",
    "                m.train(True)\n",
    "                m.inference = False\n",
    "        \n",
    "        \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        m.train(False)\n",
    "        m.inference = False\n",
    "        raw_pred = m(torch.tensor(np.array(x_train)).long(),torch.tensor(np_x_sorted_lens).long() )\n",
    "        train_pred = np.array(raw_pred.long()).argmax(axis=1)\n",
    "        train_mic_f1 = f1_score(y_train, train_pred, average='micro')\n",
    "        train_mac_f1 = f1_score(y_train, train_pred, average='macro')\n",
    "        loss = criterion(raw_pred, torch.tensor(y_train2).long())\n",
    "\n",
    "        \n",
    "        dev_pred = np.array(m(np_X_dev, np_X_dev_len)).argmax(axis=1)\n",
    "\n",
    "        dev_batch_y = y_dev\n",
    "        dev_mic_f1 = f1_score(y_dev, dev_pred, average='micro')\n",
    "        dev_mac_f1 = f1_score(y_dev, dev_pred, average='macro')\n",
    "#                 precision = precision_score(batch_y, batch_pred, average='micro')\n",
    "#                 recall = recall_score(batch_y, batch_pred, average='micro')\n",
    "        print('EPOCH SUMMARY:')\n",
    "        print(f'training loss {loss.item():.3f}\\titeraton: { iteration}\\tepoch {epoch} ')\n",
    "        print('Train:')      \n",
    "        print(f'\\tmicro f1 { train_mic_f1:.3f} macro f1 {train_mac_f1:.3f}')\n",
    "        print('Dev:')      \n",
    "        print(f'\\tmicro f1 { dev_mic_f1:.3f} macro f1 {dev_mac_f1:.3f}\\n')\n",
    "#                 print('\\tPrediction counts:')\n",
    "#                 uniques = np.unique(train_batch_pred, axis=0, return_counts=True)\n",
    "#                 print('\\tpred indeces')\n",
    "#                 print(uniques[0])\n",
    "#                 print(uniques[0][0])\n",
    "#                 print(len(uniques[0]))\n",
    "#                 print('\\t\\t',end='')\n",
    "#                 [print(str(uniques[0][i])+'  ', end='') for i in range(len(uniques[0]))]\n",
    "#                 print()\n",
    "#                 print('\\tpred counts')\n",
    "#                 print('\\t\\t',end='')\n",
    "#                 [print(str(uniques[1][i])+'  ', end='') for i in range(len(uniques[0]))]\n",
    "#                 print()\n",
    "#                 print('\\ttrue indeces')\n",
    "#                 print('\\t\\t',end='')\n",
    "#                 true_unqs = np.unique(batch_y, axis=0, return_counts=True)\n",
    "#                 [print(str(true_unqs[0][i])+'  ', end='') for i in range(len(true_unqs[0]))]\n",
    "#                 print()\n",
    "#                 print('\\ttrue counts')\n",
    "#                 print('\\t\\t',end='')\n",
    "#                 [print(str(true_unqs[1][i])+'  ', end='') for i in range(len(true_unqs[0]))]\n",
    "#                 print()\n",
    "#                 \n",
    "        m.train(True)\n",
    "        m.inference = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Describe your solution.\n",
    "\n",
    "**ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA CREDIT 1 (10 points)\n",
    "\n",
    "Modify the LSTM model to train a language model, then write code to generate new text from the model. Do not forget to mask the loss function when training the language model to handle the different lengths of the sequences. Use the \"en-ud-train.upos.tsv\" dataset.\n",
    "\n",
    "Generate 10 examples from your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPY AND EDIT CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
