{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PS3: Neural Networks for Classification and Natural Language Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this task is to gain an understanding of training neural networks. Likewise, you will get to learn about the pytorch framework.\n",
    "\n",
    "## Submission Instructions\n",
    "\n",
    "After completing the exercises below, generate a pdf of the code **with** outputs. After that create a zip file containing both the completed exercise and the generated PDF. You are **required** to check the PDF to make sure all the code **and** outputs are clearly visible and easy to read. If your code goes off the page, you should reduce the line size. I generally recommend not going over 80 characters.\n",
    "\n",
    "Finally, name the zip file using a combination of your the assigment and your name, e.g., ps3_rios.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART I: Data Cleaning (10 points)\n",
    "\n",
    "Load the \"surnames.csv\" file to train a LSTM to predict nationality based on surname. You will need to transform the data from a list of strings to a list of indexes. For example, the following data\n",
    "\n",
    "```\n",
    "Anthony\n",
    "John\n",
    "David\n",
    "```\n",
    "\n",
    "should be transformed into a list of lists.\n",
    "\n",
    "```\n",
    "[[0, 1, 2, 3, 4, 1, 5],\n",
    " [6, 4, 3, 1],\n",
    " [7, 8, 9, 10, 11]]\n",
    "```\n",
    "\n",
    "Next, you will need zero-pad all examples to be the same size.\n",
    "\n",
    "```\n",
    "[[0, 1, 2, 3, 4, 1, 5],\n",
    " [6, 4, 3, 1, 0, 0, 0],\n",
    " [7, 8, 9, 10, 11, 0, 0]]\n",
    "```\n",
    "\n",
    "Finally, everything will be converted into numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2index = {'<PAD>': 0}\n",
    "index2char = {0: '<PAD>'}\n",
    "class2index = {} # stores the class index pairs.\n",
    "index2class = {}\n",
    "doc_lengths = [] # Stores the lengths of all docs (train, test and dev)\n",
    "X_train = [] # stores an \n",
    "y_train = [] # stores an index to the correct class\n",
    "X_dev = []\n",
    "y_dev = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "X_train_len = [] # Stores the length of each training name\n",
    "X_test_len = [] # ... length of each test name\n",
    "X_dev_len = [] # ... length of each dev name\n",
    "\n",
    "# Write code to load data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PADDING\n",
    "\n",
    "max_seq_len = max(doc_lengths)\n",
    "X_train_eq_size = []\n",
    "X_dev_eq_size = []\n",
    "X_test_eq_size = []\n",
    "\n",
    "# Write code to append data to code here\n",
    "# for x in X_train:\n",
    "#     X_train_eq_size(padded x)\n",
    "    \n",
    "X_train = np.array(X_train_eq_size)\n",
    "X_dev = np.array(X_dev_eq_size)\n",
    "X_test = np.array(X_test_eq_size)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_dev = np.array(y_dev)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "X_train_len = np.array(X_train_len)\n",
    "X_dev_len = np.array(X_dev_len)\n",
    "X_test_len = np.array(X_test_len)\n",
    "\n",
    "idx = np.argsort(X_dev_len)[::-1]\n",
    "X_dev = X_dev[idx]\n",
    "y_dev = y_dev[idx]\n",
    "X_dev_len = X_dev_len[idx]\n",
    "\n",
    "idx = np.argsort(X_test_len)[::-1]\n",
    "X_test = X_test[idx]\n",
    "y_test = y_test[idx]\n",
    "X_test_len = X_test_len[idx]\n",
    "\n",
    "doc_lengths = np.array(doc_lengths)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART II: Classification (25 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, nb_layers, word2index, class2index, nb_lstm_units=100,\n",
    "                 embedding_dim=3, batch_size=3):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.vocab = word2index\n",
    "        self.tags = class2index\n",
    "\n",
    "        self.nb_layers = nb_layers\n",
    "        self.nb_lstm_units = nb_lstm_units\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.nb_tags = len(self.tags)\n",
    "\n",
    "        # build actual NN\n",
    "        self.__build_model()\n",
    "\n",
    "    def __build_model(self):\n",
    "        nb_vocab_words = len(self.vocab)\n",
    "\n",
    "        padding_idx = self.vocab['<PAD>']\n",
    "        self.word_embedding = nn.Embedding(\n",
    "            num_embeddings=nb_vocab_words,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.nb_lstm_units,\n",
    "            num_layers=self.nb_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.hidden_to_tag = nn.Linear(self.nb_lstm_units*self.nb_layers, self.nb_tags)\n",
    "        \n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.inference = False\n",
    "\n",
    "    def init_hidden(self, X):\n",
    "        # Initial ht (hidden state) and ct (context)\n",
    "        h0 = torch.zeros(self.nb_layers, X.size(0), self.nb_lstm_units).float()\n",
    "        c0 = torch.zeros(self.nb_layers, X.size(0), self.nb_lstm_units).float()\n",
    "        return (h0,c0)\n",
    "\n",
    "    def forward(self, X, X_lengths):\n",
    "        # reset the LSTM hidden state. Must be done before you run a new batch.\n",
    "        # Otherwise the LSTM will treat\n",
    "        # a new batch as a continuation of a sequence\n",
    "        self.hidden = self.init_hidden(X)\n",
    "\n",
    "        batch_size, seq_len = X.shape\n",
    "        \n",
    "        # ---------------------\n",
    "        # 1. embed the input\n",
    "        # Dim transformation: (batch_size, seq_len) -> (batch_size, seq_len, embedding_dim)\n",
    "        X = self.word_embedding(X)\n",
    "\n",
    "        # ---------------------\n",
    "        # 2. Run through RNN\n",
    "        # TRICK 2 ********************************\n",
    "        # Dim transformation: (batch_size, seq_len, embedding_dim) -> (batch_size, seq_len, nb_lstm_units)\n",
    "\n",
    "        # pack_padded_sequence so that padded items in the sequence won't be shown to the LSTM\n",
    "        X = torch.nn.utils.rnn.pack_padded_sequence(X, X_lengths, batch_first=True)\n",
    "\n",
    "        # now run through LSTM\n",
    "        # X contains the padded sequence output and ht contains the final hidden states\n",
    "        # for example.\n",
    "        X, (ht, ct) = self.lstm(X, self.hidden)\n",
    "        \n",
    "        # Reshape to use the final state from each lstm layer\n",
    "        out = ht.view(ht.size(1), self.nb_lstm_units*self.nb_layers)\n",
    "\n",
    "        # pass final states to output layer\n",
    "        out = self.hidden_to_tag(out)\n",
    "        \n",
    "        # Use logsoftmax for training and softmax for testing\n",
    "        if not self.inference:\n",
    "            Y_hat = self.logsoftmax(out)\n",
    "        else:\n",
    "            Y_hat = self.softmax(out)\n",
    "\n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "batch_size = 16\n",
    "lstm_unit_size = 512\n",
    "embedding_size = 512\n",
    "print_iter = 128\n",
    "num_layers = 1\n",
    "\n",
    "m = LSTM(num_layers, char2index, class2index, nb_lstm_units = lstm_unit_size,\n",
    "         embedding_dim = embedding_size, batch_size = batch_size)\n",
    "\n",
    "criterion = nn.NLLLoss(size_average=False)\n",
    "optim = torch.optim.Adam(m.parameters(), lr=0.001)\n",
    "\n",
    "idx = np.arange(X_train.shape[0])     \n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    np.random.shuffle(idx)\n",
    "    x_train = X_train[idx]\n",
    "    y_train = y_train[idx]\n",
    "    x_lens = X_train_len[idx]\n",
    "        \n",
    "    current_batch = 0\n",
    "    for iteration in range(y_train.shape[0] // batch_size):\n",
    "        \n",
    "        batch_lengths = x_lens[current_batch: current_batch + batch_size]\n",
    "        lengths = np.array(batch_lengths)\n",
    "        idx = np.argsort(lengths)[::-1]\n",
    "        batch_lengths = batch_lengths[idx]\n",
    "        batch_lengths = torch.tensor(batch_lengths).long()\n",
    "        \n",
    "        \n",
    "        batch_x = X_train[current_batch: current_batch + batch_size]\n",
    "        batch_x = batch_x[idx]\n",
    "        batch_x = torch.tensor(batch_x).long()\n",
    "        \n",
    "        batch_y = y_train[current_batch: current_batch + batch_size]\n",
    "        batch_y = batch_y[idx]\n",
    "        batch_y = torch.tensor(batch_y).long()\n",
    "        \n",
    "        current_batch += batch_size\n",
    "                        \n",
    "        optim.zero_grad()\n",
    "        if len(batch_x) > 0:\n",
    "            #batch_pred, batch_y = get_prediction(batch_x, batch_y)\n",
    "            batch_pred = m(batch_x, batch_lengths)\n",
    "            loss = criterion(batch_pred, batch_y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        if iteration % print_iter == 0 or True:\n",
    "            with torch.no_grad():\n",
    "                m.train(False)\n",
    "                m.inference = False\n",
    "                X_dev = torch.tensor(X_dev).long()\n",
    "                X_dev_len = torch.tensor(X_dev_len).long()\n",
    "                batch_pred = np.array(m(X_dev, X_dev_len)).argmax(axis=1)\n",
    "                batch_y = y_dev\n",
    "                f1 = f1_score(batch_y, batch_pred, average='micro')\n",
    "                #precision = precision_score(batch_y, batch_pred, average='micro')\n",
    "                #recall = recall_score(batch_y, batch_pred, average='micro')\n",
    "                print(loss.item(), '\\titeraton:', iteration, '\\tepoch', epoch, 'f1', f1)\n",
    "                m.train(True)\n",
    "                m.inference = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions below:\n",
    "\n",
    "1. What was the micro and macro F1 on the test and dev sets?\n",
    "2. Implement a bidirectional LSTM model. You will need to modify the hidden states and self.lstm variables. Does it work better?\n",
    "3. Experiments with the various hyperparameters (hidden state size, learning rate, etc.). What hyperparemeters result in the best performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART III: Natural Language Inference (25 points)\n",
    "\n",
    "Natural language inference is the task of determining whether a \"hypothesis\" is true (entailment), false (contradiction), or undetermined (neutral) given a \"premise\"[1, 2]. This task has been known to perform well for zero-shot classification[3].\n",
    "\n",
    "Example:\n",
    "\n",
    "| Premise | Label | Hypothesis |\n",
    "| ------- | ----- | ---------- |\n",
    "| A man inspects the uniform of a figure in some East Asian country. | contradiction | The man is sleeping. |\n",
    "| An older and younger man smiling | neutral | Two men are smiling and laughing at the cats playing on the floor. |\n",
    "| A soccer game with multiple males playing. | entailment | Some men are playing a sport. |\n",
    "\n",
    "Your task is to load and train a model on the \"multinli_1.0_train.jsonl\" dataset and evaluate on \"multinli_1.0_dev_matched.jsonl\" using accuracy.\n",
    "\n",
    "I am leaving this task relativley open. One solution is to modify the LSTM code above to pass two documents through a LSTM model and return the last hidden state for each. Next, concatenate the two vectors, then pass it through a softmax layer. Finally, train using the same forumlate as Part I.\n",
    "\n",
    "**NOTE:** You do not need to train until convergence. You can train for only an epoch or 2 max; train less if it takes to long. I simply want to see that it runs and is learning.\n",
    "\n",
    "\n",
    "[1] Williams, Adina, Nikita Nangia, and Samuel Bowman. \"A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference.\" Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). 2018.\n",
    "\n",
    "[2] Bowman, Samuel R., et al. \"A large annotated corpus for learning natural language inference.\" Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. 2015.\n",
    "\n",
    "[3] Yin, Wenpeng, Jamaal Hay, and Dan Roth. \"Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach.\" Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPY AND EDIT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Describe your solution.\n",
    "\n",
    "**ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA CREDIT 1 (10 points)\n",
    "\n",
    "Modify the LSTM model to train a language model, then write code to generate new text from the model. Do not forget to mask the loss function when training the language model to handle the different lengths of the sequences. Use the \"en-ud-train.upos.tsv\" dataset.\n",
    "\n",
    "Generate 10 examples from your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPY AND EDIT CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
